{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tennant/anaconda3/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "import os, time\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "\n",
    "from utils import *\n",
    "import vgg\n",
    "\n",
    "from models import dcgan_decoder, vanilla_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##params\n",
    "dataset = 'CelebA'\n",
    "save_step = 500\n",
    "sample_step = 500\n",
    "sample_size = 64\n",
    "\n",
    "train_size = np.inf\n",
    "num_epochs = 25\n",
    "\n",
    "batch_size = 64\n",
    "original_size = 108\n",
    "is_crop = True\n",
    "input_size = 64\n",
    "c_dim = 3\n",
    "l = 0.5 # weight between pixel and perceptual loss\n",
    "lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pathes\n",
    "vgg_path = './checkpoints/imagenet-vgg-verydeep-19.mat'\n",
    "checkpoint_dir = './checkpoints/'\n",
    "dcgan_path = './checkpoints/dcgan/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  encoder/d/in: (64, 64, 64, 3)\n",
      "[TL] Conv2dLayer encoder/enc/h0/conv2d: shape:[5, 5, 3, 64] strides:[1, 2, 2, 1] pad:SAME act:<lambda>\n",
      "[TL] Conv2dLayer encoder/enc/h1/conv2d: shape:[5, 5, 64, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "[TL] BatchNormLayer encoder/enc/h1/bn: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "[TL] Conv2dLayer encoder/enc/h2/conv2d: shape:[5, 5, 128, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Model\n",
      "Input Shape:  (64, 64, 64, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] BatchNormLayer encoder/enc/h2/bn: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "[TL] Conv2dLayer encoder/enc/h3/conv2d: shape:[5, 5, 256, 512] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "[TL] BatchNormLayer encoder/enc/h3/bn: decay:0.900000 epsilon:0.000010 act:<lambda> is_train:True\n",
      "[TL] FlattenLayer encoder/enc/h4/flatten: 8192\n",
      "[TL] DenseLayer  encoder/enc/h4/lin: 100 identity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent Shape:  (64, 100)\n"
     ]
    }
   ],
   "source": [
    "## Define model\n",
    "print('Building Model')\n",
    "\n",
    "input_img = tf.placeholder(tf.float32, shape=[batch_size, input_size, input_size, c_dim], name='input_img')\n",
    "print('Input Shape: ', input_img.get_shape())\n",
    "encoder_net, _ = vanilla_encoder.model(input_img, z_dim=100)\n",
    "print('Latent Shape: ', encoder_net.outputs.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[TL] InputLayer  generator/g/in: (64, 100)\n",
      "[TL] DenseLayer  generator/g/h0/lin: 8192 identity\n",
      "[TL] ReshapeLayer generator/g/h0/reshape: (64, 4, 4, 512)\n",
      "[TL] BatchNormLayer generator/g/h0/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] DeConv2dLayer generator/g/h1/decon2d: shape:[5, 5, 256, 512] out_shape:[64, 8, 8, 256] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "[TL] BatchNormLayer generator/g/h1/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] DeConv2dLayer generator/g/h2/decon2d: shape:[5, 5, 128, 256] out_shape:[64, 16, 16, 128] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "[TL] BatchNormLayer generator/g/h2/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] DeConv2dLayer generator/g/h3/decon2d: shape:[5, 5, 64, 128] out_shape:[64, 32, 32, 64] strides:[1, 2, 2, 1] pad:SAME act:identity\n",
      "[TL] BatchNormLayer generator/g/h3/batch_norm: decay:0.900000 epsilon:0.000010 act:relu is_train:False\n",
      "[TL] DeConv2dLayer generator/g/h4/decon2d: shape:[5, 5, 3, 64] out_shape:[64, 64, 64, 3] strides:[1, 2, 2, 1] pad:SAME act:identity\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape:  (64, 64, 64, 3)\n",
      "Successfully built!\n"
     ]
    }
   ],
   "source": [
    "decoder_net, _ = dcgan_decoder.model(encoder_net.outputs, image_size=input_size, c_dim=c_dim, batch_size=batch_size)\n",
    "print('Output Shape: ', decoder_net.outputs.get_shape())\n",
    "print('Successfully built!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define loss and training ops\n",
    "loss_pixel = tf.nn.l2_loss(input_img - decoder_net.outputs) / (batch_size*input_size*input_size*c_dim)\n",
    "\n",
    "vgg_net = vgg.VGG(vgg_path)\n",
    "loss_calc = LossCalculator(vgg_net, decoder_net.outputs)\n",
    "loss_perc = loss_calc.content_loss(input_img, content_layer='relu4_3', content_weight=1) / batch_size\n",
    "loss = l * loss_pixel + (1 - l) * loss_perc\n",
    "train_param = encoder_net.all_params + decoder_net.all_params\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, var_list=train_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# open up a tf.Session\n",
    "sess = tf.InteractiveSession()\n",
    "tl.layers.initialize_global_variables(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loading pretrained param\n",
    "print('Loading trained parameters of decoder network...')\n",
    "decoder_params = tl.files.load_npz(name=dcgan_path+'net_g.npz')\n",
    "tl.files.assign_params(sess, decoder_params, decoder_net)\n",
    "print('Successfully loaded trained parameters of the decoder network')\n",
    "decoder_net.print_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Set path to save param and get training data\n",
    "\n",
    "\n",
    "\n",
    "data_files = glob(os.path.join(\"./data\", dataset, \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the model up and training\n",
    "iter_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    shuffle(data_files)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
